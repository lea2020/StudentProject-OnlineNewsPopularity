---
title: "FinalResults_02-ExploratoryAnalysis"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Predicting Online News Popularity based on Pre-publication Features

Final Results and Code


Step 2 - Exploratory Analysis

```{r }
#Reload the data
reduced_pop <- read.csv(file = "reduced_pop.csv", header = TRUE, sep = ",", stringsAsFactors = FALSE, 
                        na.strings = c("", "NA"))

#Remove the index column
reduced_pop <- reduced_pop[2:45]
```

```{r }
#Change the categorical variables back to factor to conduct exploratory analysis
reduced_pop$data_channel_is_lifestyle <- as.factor(reduced_pop$data_channel_is_lifestyle)
reduced_pop$data_channel_is_entertainment <- as.factor(reduced_pop$data_channel_is_entertainment)
reduced_pop$data_channel_is_bus <- as.factor(reduced_pop$data_channel_is_bus)
reduced_pop$data_channel_is_socmed <- as.factor(reduced_pop$data_channel_is_socmed)
reduced_pop$data_channel_is_tech <- as.factor(reduced_pop$data_channel_is_tech)
reduced_pop$data_channel_is_world <- as.factor(reduced_pop$data_channel_is_world)
reduced_pop$weekday_is_monday <- as.factor(reduced_pop$weekday_is_monday)
reduced_pop$weekday_is_tuesday <- as.factor(reduced_pop$weekday_is_tuesday)
reduced_pop$weekday_is_wednesday <- as.factor(reduced_pop$weekday_is_wednesday)
reduced_pop$weekday_is_thursday <- as.factor(reduced_pop$weekday_is_thursday)
reduced_pop$weekday_is_friday <- as.factor(reduced_pop$weekday_is_friday)
reduced_pop$weekday_is_saturday <- as.factor(reduced_pop$weekday_is_saturday)
reduced_pop$weekday_is_sunday <- as.factor(reduced_pop$weekday_is_sunday)
reduced_pop$shares_cat <- as.factor(reduced_pop$shares_cat)
```

Normalization is applied to the dataset to scale the values of the features. Next, the data may be explored by checking subsequences using association rules to find any patterns that may explain what makes an article popular, and learn its characteristics, without making predictions yet.

```{r }
#Normalization - normalize the numeric features in the data set
#Use a function to normalize the data
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

#Determine the categorical variables
categorical = apply(reduced_pop, 2, function(x){all(x %in% 0:1)})
print(categorical)
```
```{r }
#Apply the normalize function to the numeric variables
pop_norm <- as.data.frame(lapply(reduced_pop[!categorical], normalize))

#Combine the numeric and categorical variables back into one data set 
pop_norm <- cbind(pop_norm, reduced_pop[categorical])

#Check the five number summary and frequency tables for the normalized dataset
summary(pop_norm)
```

Check subsequences - Association rules

```{r }
#install.packages("arules")
library(arules)
#install.packages("arulesViz")
library(arulesViz)
#install.packages("plyr")
library(plyr)
#install.packages("dplyr")
library(dplyr)
#install.packages("arulesCBA")
library(arulesCBA)
```

Extract rules using apriori algorithm, setting minimum support and confidence.
```{r }
#First, subset the data to only those that are popular
mostpopular <- subset(pop_norm, pop_norm$shares_cat == 1)

#Remove the popularity level of shares column since all the observations are for popular articles
mostpopular <- mostpopular[-44]

#Change the dataframe into transaction data
transaction_object <- as(mostpopular, "transactions")

#Check the transaction data
glimpse(transaction_object)
```
```{r }
summary(transaction_object)
```
```{r }
#Create an item frequency plot to view the distributions for the top 20 items
if (!require("RColorBrewer")) {
  #Install the color package of R
  install.packages("RColorBrewer")
  #Use the library RColorBrewer
  library(RColorBrewer)
}

#Absolute Item Frequency Plot
itemFrequencyPlot(transaction_object, topN=20, type="absolute", col=brewer.pal(8,'Pastel2'), 
                  main="Absolute Item Frequency Plot")
```
```{r }
#Relative Item Frequency Plot
itemFrequencyPlot(transaction_object, topN=20, type="relative", col=brewer.pal(8,'Pastel2'), 
                  main="Relative Item Frequency Plot")
```

The most frequently occurring items are the maximum shares of the best keyword followed by whether the data channel is lifestyle, the data channel is social media, the article was posted on Saturday, the article was posted on Sunday, the data channel is entertainment, the article was posted on Friday, data channel is world, data channel is business, the article was posted on Monday, and so on.

```{r }
#Create association rules using the apriori algorithm
#Use minimum support = 0.5, and minimum confidence = 0.9
association.rules <- apriori(transaction_object, parameter = list(supp=0.5, conf=0.9, minlen=2))

summary(association.rules)
```
```{r }
#Check the first 10 rules
inspect(association.rules[1:10])
```
```{r }
#Sort by rules with high lift
rules_lift <- sort(association.rules, by = "lift", decreasing = TRUE)

#Show the support, lift, and confidence for the top 30 rules with high lift
inspect(rules_lift[1:30])
```

Some interesting rules found that may help determine whether an article will be popular include:

51% of the transactions show that 91% of popular articles with the number of videos between 0 and the bottom 1% of the distribution, maximum shares of the best keyword, and not published on Sunday (or Saturday), are also not entertainment.

50-52% of the transactions show that 90% of popular articles with the number of videos between 0 and the bottom 1% of the distribution, maximum shares of the best keyword, and not social media (or lifestyle), are also not entertainment.

51% of the transactions show that 95% of popular articles with maximum shares of the best keyword, with the top 90% of the minimum polarity of positive words, and not published on Saturday (or Sunday), are also not social media.

52% of the transactions show that 95% of popular articles with maximum shares of the best keyword, with the top 90% of the minimum polarity of positive words, and not lifestyle, are also not social media.

50% of transactions show that 92% of popular articles with maximum shares of the best keyword, absolute subjectivity level in the top 53% of the distribution, and not lifestyle, or entertainment, are also not published on Sunday.

```{r }
#Plot the rules
plot(rules_lift, main = "Scatterplot for Association Rules")
#Rules with high lift have low support.
```

```{r }
#Two-key plot
#The order shows the number of items in a rule
plot(rules_lift, method = "two-key plot")
#There are more items for rules with lower support.
```

```{r }
#Interactive scatter-plot
#Hover over each rule to show the items and quality measures (support, confidence, lift)
plotly_arules(rules_lift)
```

```{r }
#Interactive graph-based visualization
#Filter the top 20 rules with highest lift
subset_rules <- head(association.rules, n=20, by="lift", decreasing = TRUE)
plot(subset_rules, method = "graph",  engine = "htmlwidget")
```

```{r }
#Individual rule representation - parallel coordinates plot
#Visualize what items cause other items in the set
plot(subset_rules, method="paracoord")
```

The right-hand side (RHS)/consequent is the item the set is proposed to have, and on the left-hand side are the most recent additions to the set.

```{r }
#Save the dataframe
write.csv(pop_norm, "pop_norm.csv")
```
